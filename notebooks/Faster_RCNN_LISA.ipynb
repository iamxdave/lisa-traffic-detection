{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3a99fa1c",
      "metadata": {},
      "source": [
        "# Traffic Light Detection using Faster R-CNN on the LISA Dataset (Combined Day/Night)\n",
        "\n",
        "Implementation and training of the Faster R-CNN model. Models trained on combined day and night data.\n",
        "Version with MetricLogger fixes, Early Stopping, P/R/F1 extraction from COCO, plotting graphs, and saving results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0876c7c",
      "metadata": {},
      "source": [
        "## 0. Import Diagnostics and Path Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77090cda",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"--- Import Path Diagnostics ---\")\n",
        "current_working_directory = os.getcwd()\n",
        "print(f\"Current working directory (CWD): {current_working_directory}\")\n",
        "\n",
        "project_root_directory = os.path.dirname(current_working_directory)\n",
        "print(f\"Parent directory (potential project_root): {project_root_directory}\")\n",
        "\n",
        "if project_root_directory not in sys.path:\n",
        "    sys.path.insert(0, project_root_directory)\n",
        "    print(f\"Added to sys.path: {project_root_directory}\")\n",
        "else:\n",
        "    print(f\"Directory {project_root_directory} is already in sys.path.\")\n",
        "\n",
        "expected_utils_path_absolute = os.path.join(project_root_directory, 'utils')\n",
        "expected_utils_path_absolute = os.path.abspath(expected_utils_path_absolute)\n",
        "print(f\"Expected absolute path to utils folder: {expected_utils_path_absolute}\")\n",
        "print(f\"Does the utils folder exist at the expected location? {os.path.isdir(expected_utils_path_absolute)}\")\n",
        "if os.path.isdir(expected_utils_path_absolute):\n",
        "    print(f\"Contents of the utils folder: {os.listdir(expected_utils_path_absolute)}\")\n",
        "\n",
        "print(\"--- End of Diagnostics ---\")\n",
        "\n",
        "print(\"\\nAttempting to import tools...\")\n",
        "COCO_UTILS_AVAILABLE = False\n",
        "METRIC_LOGGER_AVAILABLE = False \n",
        "\n",
        "try:\n",
        "    from utils.coco_eval import CocoEvaluator\n",
        "    from utils.coco_utils import get_coco_api_from_dataset\n",
        "    COCO_UTILS_AVAILABLE = True\n",
        "    print(\"SUCCESS: COCO tools imported successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"COCO TOOLS IMPORT ERROR: {e}.\")\n",
        "    if 'CocoEvaluator' not in globals():\n",
        "        class CocoEvaluator:\n",
        "            def __init__(self, coco_gt, iou_types): self.coco_gt = coco_gt; self.iou_types = iou_types; self.eval_imgs = []\n",
        "            def update(self, predictions): pass\n",
        "            def synchronize_between_processes(self): pass\n",
        "            def accumulate(self): pass\n",
        "            def summarize(self): print(\"Mock CocoEvaluator.summarize() used: mAP is not calculated correctly.\")\n",
        "    if 'get_coco_api_from_dataset' not in globals():\n",
        "        def get_coco_api_from_dataset(dataset): print(\"Mock get_coco_api_from_dataset() used.\"); return None\n",
        "\n",
        "try:\n",
        "    from utils.utils import MetricLogger, SmoothedValue \n",
        "    METRIC_LOGGER_AVAILABLE = True\n",
        "    print(\"SUCCESS: MetricLogger and SmoothedValue imported successfully from utils.utils.\")\n",
        "except ImportError as e:\n",
        "    print(f\"IMPORT ERROR from utils.utils: {e}.\")\n",
        "    if 'MetricLogger' not in globals(): \n",
        "        class MetricLogger:\n",
        "            def __init__(self, delimiter=None): self.meters = {}; self.delimiter = delimiter; print(\"WARNING: Mock MetricLogger used.\")\n",
        "            def add_meter(self, name, meter): self.meters[name] = meter \n",
        "            def update(self, **kwargs): pass\n",
        "            def synchronize_between_processes(self): pass\n",
        "            def __str__(self): return \"MockedMetricLogger\"\n",
        "            def log_every(self, iterable, print_freq, header=None):\n",
        "                if header: print(header)\n",
        "                from tqdm.auto import tqdm \n",
        "                for i, data_batch in enumerate(tqdm(iterable, desc=header if header else \"Iteration\", leave=False)):\n",
        "                    yield data_batch\n",
        "    if 'SmoothedValue' not in globals():\n",
        "        class SmoothedValue: \n",
        "            def __init__(self, window_size=20, fmt=None): self.deque = []; self.fmt = fmt\n",
        "            def update(self, value, n=1): self.deque.append(value) \n",
        "            def __str__(self): import numpy; return str(numpy.mean(self.deque)) if self.deque else \"N/A\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6866ae36",
      "metadata": {},
      "source": [
        "## 1. Imports and Basic Configuration (continued)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7c6d0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import time\n",
        "import cv2 \n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import json \n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "CLASS_MAPPING = {'background': 0, 'go': 1, 'stop': 2, 'warning': 3, 'off': 4}\n",
        "INV_CLASS_MAPPING = {v: k for k, v in CLASS_MAPPING.items()} \n",
        "NUM_CLASSES_INC_BG = len(CLASS_MAPPING)\n",
        "NUM_CLASSES_NO_BG = NUM_CLASSES_INC_BG - 1 \n",
        "CLASS_NAMES_NO_BG = [INV_CLASS_MAPPING[i] for i in range(1, NUM_CLASSES_INC_BG)]\n",
        "IMAGE_SIZE = 640 \n",
        "\n",
        "BASE_DATA_PATH = \"../dataset/lisa_traffic_light_dataset/\"\n",
        "ANNOTATIONS_DIR = os.path.join(BASE_DATA_PATH, \"annotations\")\n",
        "IMAGES_BASE_DIR = os.path.join(BASE_DATA_PATH, \"images\") \n",
        "\n",
        "TRAIN_ANNOTATIONS_FILE = os.path.join(ANNOTATIONS_DIR, \"train_annotations.csv\")\n",
        "VAL_ANNOTATIONS_FILE = os.path.join(ANNOTATIONS_DIR, \"val_annotations.csv\")\n",
        "\n",
        "OUTPUT_DIR = \"../results/faster_rcnn/\"\n",
        "VISUALIZATIONS_DIR = os.path.join(OUTPUT_DIR, \"visualizations\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(VISUALIZATIONS_DIR, exist_ok=True)\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 0.001 \n",
        "WEIGHT_DECAY = 0.0005 \n",
        "MOMENTUM = 0.9\n",
        "EARLY_STOPPING_PATIENCE = 10 \n",
        "PRINT_FREQ = 10 \n",
        "\n",
        "USE_SUBSET_DATA = True \n",
        "SUBSET_SIZE_TRAIN = 50\n",
        "SUBSET_SIZE_VAL = 20 \n",
        "APPLY_AUGMENTATIONS = False\n",
        "\n",
        "TRAINING_ARGS = {\n",
        "    \"model_name\": \"FasterRCNN_ResNet50_FPN\", \"num_epochs\": NUM_EPOCHS, \"batch_size\": BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE, \"weight_decay\": WEIGHT_DECAY, \"momentum\": MOMENTUM,\n",
        "    \"early_stopping_patience\": EARLY_STOPPING_PATIENCE, \"image_size\": IMAGE_SIZE,\n",
        "    \"use_subset_data\": USE_SUBSET_DATA,\n",
        "    \"subset_size_train\": SUBSET_SIZE_TRAIN if USE_SUBSET_DATA else -1,\n",
        "    \"subset_size_val\": SUBSET_SIZE_VAL if USE_SUBSET_DATA else -1,\n",
        "    \"class_mapping\": {k:v for k,v in CLASS_MAPPING.items()},\n",
        "    \"apply_augmentations\": APPLY_AUGMENTATIONS\n",
        "}\n",
        "args_save_path = os.path.join(OUTPUT_DIR, 'training_arguments_faster_rcnn.json')\n",
        "try:\n",
        "    with open(args_save_path, 'w') as f:\n",
        "        json.dump(TRAINING_ARGS, f, indent=4)\n",
        "    print(f\"Training arguments saved to {args_save_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving training arguments: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e93229",
      "metadata": {},
      "source": [
        "## 2. `LISADataset` Class and Augmentations (with filename_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366baf01",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LISADataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_base_dir, transforms=None, class_mapping=None):\n",
        "        try:\n",
        "            self.full_annotations_df = pd.read_csv(annotations_file)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"CRITICAL ERROR: Annotation file {annotations_file} not found!\")\n",
        "            self.full_annotations_df = pd.DataFrame(columns=['filename', 'xmin', 'ymin', 'xmax', 'ymax', 'label'])\n",
        "            \n",
        "        self.img_base_dir = img_base_dir\n",
        "        self.transforms = transforms\n",
        "        self.class_mapping = class_mapping\n",
        "        \n",
        "        if self.full_annotations_df.empty:\n",
        "             self.image_filenames = np.array([]) \n",
        "        else:\n",
        "            self.image_filenames = self.full_annotations_df['filename'].unique()\n",
        "        \n",
        "        self.filename_to_id = {fname: i for i, fname in enumerate(self.image_filenames)}\n",
        "        \n",
        "        self.image_annotations = {\n",
        "            filename: self.full_annotations_df[self.full_annotations_df['filename'] == filename]\n",
        "            for filename in self.image_filenames\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= len(self.image_filenames):\n",
        "            return None, None \n",
        "            \n",
        "        img_relative_path = self.image_filenames[idx]\n",
        "        img_path = os.path.join(self.img_base_dir, img_relative_path)\n",
        "        \n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except FileNotFoundError: \n",
        "            return None, None \n",
        "        except Exception as e_img_open:\n",
        "            print(f\"ERROR (LISADataset): Could not open image {img_path}: {e_img_open}\")\n",
        "            return None, None\n",
        "\n",
        "        annots = self.image_annotations[img_relative_path]\n",
        "        boxes = annots[['xmin','ymin','xmax','ymax']].values.astype(np.float32)\n",
        "        \n",
        "        try: \n",
        "            labels_tensor = torch.tensor([self.class_mapping[lbl] for lbl in annots['label']], dtype=torch.int64)\n",
        "        except KeyError as e: \n",
        "            labels_tensor = torch.zeros(0,dtype=torch.int64) \n",
        "            boxes = np.zeros((0,4),dtype=np.float32)\n",
        "\n",
        "        unique_img_id = self.filename_to_id[img_relative_path]\n",
        "        target={'boxes':torch.as_tensor(boxes,dtype=torch.float32),\n",
        "                 'labels':labels_tensor,\n",
        "                 'image_id':torch.tensor([unique_img_id], dtype=torch.int64)} \n",
        "        \n",
        "        if boxes.shape[0]>0: \n",
        "            target['area']=torch.as_tensor((boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1]),dtype=torch.float32)\n",
        "        else: \n",
        "            target['area']=torch.zeros(0,dtype=torch.float32)\n",
        "            target['boxes']=torch.zeros((0,4),dtype=torch.float32)\n",
        "            target['labels']=torch.zeros(0,dtype=torch.int64)\n",
        "        \n",
        "        target['iscrowd']=torch.zeros((target['boxes'].shape[0],),dtype=torch.int64)\n",
        "\n",
        "        if self.transforms:\n",
        "            image_np = np.array(image)\n",
        "            labels_for_albumentations = target['labels'].tolist()\n",
        "            bboxes_for_albumentations = target['boxes'].numpy()\n",
        "            try:\n",
        "                transformed = self.transforms(image=image_np, bboxes=bboxes_for_albumentations, labels=labels_for_albumentations)\n",
        "                image = transformed['image'] \n",
        "                if len(transformed['bboxes']) > 0:\n",
        "                    target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "                    target['labels'] = torch.as_tensor(transformed['labels'], dtype=torch.int64)\n",
        "                else:\n",
        "                    target['boxes'] = torch.zeros((0,4), dtype=torch.float32)\n",
        "                    target['labels'] = torch.zeros(0, dtype=torch.int64)\n",
        "            except Exception as e:\n",
        "                default_transform = A.Compose([\n",
        "                    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "                    A.Normalize(mean=[.485,.456,.406],std=[.229,.224,.225]),\n",
        "                    ToTensorV2()\n",
        "                ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "                transformed = default_transform(image=image_np, bboxes=bboxes_for_albumentations, labels=labels_for_albumentations)\n",
        "                image = transformed['image']\n",
        "                if len(transformed['bboxes']) > 0:\n",
        "                    target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "                    target['labels'] = torch.as_tensor(transformed['labels'], dtype=torch.int64)\n",
        "                else:\n",
        "                    target['boxes'] = torch.zeros((0,4), dtype=torch.float32)\n",
        "                    target['labels'] = torch.zeros(0, dtype=torch.int64)\n",
        "            \n",
        "        if target['boxes'].shape[0] > 0:\n",
        "            valid_idx = (target['boxes'][:,2] > target['boxes'][:,0] + 1e-3) & (target['boxes'][:,3] > target['boxes'][:,1] + 1e-3)\n",
        "            target['boxes'] = target['boxes'][valid_idx]\n",
        "            target['labels'] = target['labels'][valid_idx]\n",
        "            if target['boxes'].shape[0] > 0: \n",
        "                target['area'] = (target['boxes'][:, 2] - target['boxes'][:, 0]) * (target['boxes'][:, 3] - target['boxes'][:, 1])\n",
        "                target['iscrowd'] = torch.zeros((target['boxes'].shape[0],), dtype=torch.int64)\n",
        "            else:\n",
        "                target['area'] = torch.zeros(0, dtype=torch.float32); target['iscrowd'] = torch.zeros(0, dtype=torch.int64)\n",
        "                target['boxes'] = torch.zeros((0,4), dtype=torch.float32); target['labels'] = torch.zeros(0, dtype=torch.int64)\n",
        "        else: \n",
        "            target['boxes'] = torch.zeros((0,4), dtype=torch.float32); target['labels'] = torch.zeros(0, dtype=torch.int64)\n",
        "            target['area'] = torch.zeros(0, dtype=torch.float32); target['iscrowd'] = torch.zeros(0, dtype=torch.int64)\n",
        "            \n",
        "        return image, target\n",
        "\n",
        "def get_train_transforms(apply_augmentations=True):\n",
        "\n",
        "    base_transforms = [\n",
        "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        "\n",
        "    if apply_augmentations:\n",
        "        augmentation_transforms = [\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
        "            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.15, rotate_limit=50, p=0.6, \n",
        "                                border_mode=cv2.BORDER_CONSTANT), \n",
        "            A.Affine(shear=(-10, 10), p=0.3),\n",
        "            A.OneOf([\n",
        "                A.GaussianBlur(blur_limit=(3, 7), p=0.5),\n",
        "                A.MotionBlur(blur_limit=7, p=0.5),\n",
        "            ], p=0.3),\n",
        "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "            A.RandomGamma(gamma_limit=(80,120), p=0.3),\n",
        "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8,8), p=0.2),\n",
        "        ]\n",
        "        all_transforms = augmentation_transforms + base_transforms\n",
        "    else:\n",
        "        all_transforms = base_transforms\n",
        "    \n",
        "    return A.Compose(all_transforms, bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_visibility=0.2, min_area=10))\n",
        "\n",
        "def get_val_test_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b is not None and b[0] is not None and b[1] is not None] \n",
        "    if not batch: \n",
        "        return None, None \n",
        "    return tuple(zip(*batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ddfd9d",
      "metadata": {},
      "source": [
        "## 3. Faster R-CNN Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5282ce25",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_faster_rcnn_model(num_classes_incl_background, backbone_name=\"resnet50\"):\n",
        "    if backbone_name == \"resnet50\":\n",
        "        model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(\n",
        "            weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported backbone for Faster R-CNN: {backbone_name}\")\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes_incl_background)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8067d7e",
      "metadata": {},
      "source": [
        "## 4. Training Loop and Evaluation (with Early Stopping and Visualizations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0aedaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch_num, print_freq=10):\n",
        "    model.train()\n",
        "    current_metric_logger = None \n",
        "    if METRIC_LOGGER_AVAILABLE:\n",
        "        current_metric_logger = MetricLogger(delimiter=\"  \")\n",
        "        current_metric_logger.add_meter(\"lr\", SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "        current_metric_logger.add_meter('loss_classifier', SmoothedValue(window_size=1, fmt='{median:.4f} ({global_avg:.4f})'))\n",
        "        current_metric_logger.add_meter('loss_box_reg', SmoothedValue(window_size=1, fmt='{median:.4f} ({global_avg:.4f})'))\n",
        "        current_metric_logger.add_meter('loss_objectness', SmoothedValue(window_size=1, fmt='{median:.4f} ({global_avg:.4f})'))\n",
        "        current_metric_logger.add_meter('loss_rpn_box_reg', SmoothedValue(window_size=1, fmt='{median:.4f} ({global_avg:.4f})'))\n",
        "    else: \n",
        "        class MockMetricLoggerForTrain:\n",
        "            def __init__(self, delimiter=None): self.meters = {}; self.delimiter = delimiter\n",
        "            def add_meter(self, name, meter): self.meters[name] = meter\n",
        "            def update(self, **kwargs): pass\n",
        "            def __str__(self): return \"MockedTrainMetricLogger\"\n",
        "            def log_every(self, iterable, print_freq, header=None):\n",
        "                from tqdm.auto import tqdm \n",
        "                return tqdm(iterable, desc=header if header else \"Training\", leave=False)\n",
        "        current_metric_logger = MockMetricLoggerForTrain(delimiter=\"  \")\n",
        "        if 'SmoothedValue' in globals() and hasattr(globals()['SmoothedValue'], '__call__'): current_metric_logger.add_meter(\"lr\", SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "            \n",
        "    header = f\"Epoch [{epoch_num}]\"\n",
        "    lr_scheduler = None \n",
        "    if epoch_num -1 == 0: \n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1) if len(data_loader) > 1 else 1\n",
        "        if warmup_iters > 0 : \n",
        "            lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=warmup_factor, total_iters=warmup_iters)\n",
        "    \n",
        "    epoch_losses = {'loss_classifier': [], 'loss_box_reg': [], 'loss_objectness': [], 'loss_rpn_box_reg': [], 'loss': []}\n",
        "\n",
        "    for images, targets in current_metric_logger.log_every(data_loader, print_freq, header):\n",
        "        if images is None or targets is None: \n",
        "            continue\n",
        "        \n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items() if isinstance(v, torch.Tensor)} for t in targets]\n",
        "        for target_item in targets: target_item['boxes'] = target_item['boxes'].float()\n",
        "        \n",
        "        try:\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "            loss_value = losses.item()\n",
        "            \n",
        "            if not np.isfinite(loss_value): \n",
        "                print(f\"Epoch: {epoch_num}, Infinite loss: {loss_value}, skipping batch.\")\n",
        "                continue \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if lr_scheduler is not None: lr_scheduler.step()\n",
        "            \n",
        "            loss_dict_items = {k: v.item() for k, v in loss_dict.items()}\n",
        "            if METRIC_LOGGER_AVAILABLE:\n",
        "                current_metric_logger.update(loss=loss_value, **loss_dict_items)\n",
        "                current_metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "            \n",
        "            epoch_losses['loss'].append(loss_value)\n",
        "            for k, v_item in loss_dict_items.items():\n",
        "                if k in epoch_losses: # Ensure key exists\n",
        "                    epoch_losses[k].append(v_item)\n",
        "\n",
        "        except Exception as e: \n",
        "            print(f\"Error in training step: {e}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "            continue \n",
        "    \n",
        "    avg_epoch_losses = {k: np.mean(v) if v else 0.0 for k, v in epoch_losses.items()}\n",
        "    \n",
        "    returned_losses = {}\n",
        "    for loss_name in epoch_losses.keys():\n",
        "        if METRIC_LOGGER_AVAILABLE and hasattr(current_metric_logger, 'meters') and loss_name in current_metric_logger.meters and hasattr(current_metric_logger.meters[loss_name], 'global_avg'):\n",
        "            returned_losses[loss_name] = current_metric_logger.meters[loss_name].global_avg\n",
        "        else:\n",
        "            returned_losses[loss_name] = avg_epoch_losses[loss_name]\n",
        "            \n",
        "    return returned_losses\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(model, data_loader, device, class_mapping, coco_utils_available_flag, metric_logger_available_flag, num_classes_no_bg_for_curves, inv_class_mapping_for_curves):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    current_metric_logger_eval = None \n",
        "    iou_threshold = 0.5\n",
        "    if metric_logger_available_flag:\n",
        "        current_metric_logger_eval = MetricLogger(delimiter=\"  \")\n",
        "        if 'SmoothedValue' in globals() and hasattr(globals()['SmoothedValue'], '__call__'):\n",
        "             current_metric_logger_eval.add_meter(\"model_time\", SmoothedValue(window_size=1, fmt=\"{value:.4f}\"))\n",
        "    else:\n",
        "        class MockMetricLoggerEval: \n",
        "            def __init__(self, delimiter=None): self.delimiter = delimiter; self.meters = {}\n",
        "            def add_meter(self, name, meter): pass\n",
        "            def update(self, **kwargs): pass\n",
        "            def synchronize_between_processes(self): pass\n",
        "            def __str__(self): return \"MockedEvalMetricLogger\"\n",
        "            def log_every(self, iterable, print_freq, header=None):\n",
        "                if header: print(header)\n",
        "                from tqdm.auto import tqdm \n",
        "                for i, data_batch in enumerate(tqdm(iterable, desc=header if header else \"Evaluating\", leave=False)):\n",
        "                    yield data_batch\n",
        "        current_metric_logger_eval = MockMetricLoggerEval(delimiter=\"  \")\n",
        "\n",
        "    header = \"Test:\"\n",
        "    actual_dataset_for_coco = data_loader.dataset \n",
        "    if isinstance(data_loader.dataset, torch.utils.data.Subset):\n",
        "        actual_dataset_for_coco = data_loader.dataset.dataset\n",
        "\n",
        "    coco = None; coco_evaluator = None\n",
        "    if coco_utils_available_flag:\n",
        "        try:\n",
        "            coco = get_coco_api_from_dataset(actual_dataset_for_coco)\n",
        "            if coco:\n",
        "                try: iou_types = _get_iou_types(model)\n",
        "                except NameError: iou_types = [\"bbox\"]\n",
        "                coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "        except Exception as e_coco_api: print(f\"ERROR (evaluate) during get_coco_api or CocoEvaluator: {e_coco_api}\")\n",
        "\n",
        "    all_predictions_for_curves = [] \n",
        "    all_targets_for_curves = []    \n",
        "    all_pred_labels_for_cm = []\n",
        "    all_gt_labels_for_cm = []\n",
        "    optimal_confidence_threshold_for_cm = 0.5\n",
        "\n",
        "    inference_times = []\n",
        "    num_processed_frames = 0\n",
        "    iterable_eval = current_metric_logger_eval.log_every(data_loader, 100, header) \n",
        "\n",
        "    for images, targets in iterable_eval:\n",
        "        if images is None or targets is None:\n",
        "            continue\n",
        "        images_for_model = list(img.to(device) for img in images)\n",
        "        targets_cpu = [{k: v.cpu().clone() if isinstance(v, torch.Tensor) else v for k,v in t.items()} for t in targets]\n",
        "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "        model_time_start = time.time(); outputs = model(images_for_model); model_time_end = time.time(); model_time = model_time_end - model_time_start\n",
        "        outputs_cpu = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        inference_times.append(model_time); num_processed_frames += len(images)\n",
        "        res = {}\n",
        "        if coco_evaluator:\n",
        "            for t_cpu, o_cpu in zip(targets_cpu, outputs_cpu):\n",
        "                if \"image_id\" in t_cpu and isinstance(t_cpu[\"image_id\"], torch.Tensor) and t_cpu[\"image_id\"].numel()==1:\n",
        "                    img_id_val = t_cpu[\"image_id\"].item()\n",
        "                    res[img_id_val] = o_cpu\n",
        "            if res: coco_evaluator.update(res)\n",
        "        \n",
        "        for i_img in range(len(outputs_cpu)):\n",
        "            all_predictions_for_curves.append(outputs_cpu[i_img]) \n",
        "            all_targets_for_curves.append(targets_cpu[i_img])    \n",
        "            \n",
        "        if metric_logger_available_flag: current_metric_logger_eval.update(model_time=model_time)\n",
        "            \n",
        "    if metric_logger_available_flag: \n",
        "        current_metric_logger_eval.synchronize_between_processes()\n",
        "        print(\"Averaged stats (MetricLogger):\", current_metric_logger_eval)\n",
        "        \n",
        "    if coco_evaluator:\n",
        "        try: coco_evaluator.synchronize_between_processes(); coco_evaluator.accumulate(); coco_evaluator.summarize()\n",
        "        except Exception as e_coco_final: print(f\"COCO finalization error: {e_coco_final}\"); import traceback; traceback.print_exc()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    metrics = {\"mAP_0.5_0.95\": -1., \"mAP_0.5\": -1., \"Precision_coco\": -1., \"Recall_coco\": -1., \"F1_coco\": -1., \"FPS\": -1.}\n",
        "    coco_eval_stats_for_curves = None \n",
        "    if coco_utils_available_flag and coco_evaluator and hasattr(coco_evaluator, 'coco_eval'):\n",
        "        for iou_type, coco_eval_obj_instance in coco_evaluator.coco_eval.items(): \n",
        "            if coco_eval_obj_instance and hasattr(coco_eval_obj_instance, 'stats') and coco_eval_obj_instance.stats is not None:\n",
        "                stats = coco_eval_obj_instance.stats\n",
        "                if iou_type == \"bbox\": \n",
        "                    metrics[\"mAP_0.5_0.95\"] = round(stats[0],4) if len(stats)>0 else -1.\n",
        "                    metrics[\"mAP_0.5\"]=round(stats[1],4) if len(stats)>1 else -1.\n",
        "                    precision_coco = metrics[\"mAP_0.5\"] \n",
        "                    recall_coco = round(stats[8], 4) if len(stats) > 8 else -1.0 \n",
        "                    metrics[\"Precision_coco\"] = precision_coco\n",
        "                    metrics[\"Recall_coco\"] = recall_coco\n",
        "                    if precision_coco > 0 and recall_coco > 0:\n",
        "                        metrics[\"F1_coco\"] = round(2 * (precision_coco * recall_coco) / (precision_coco + recall_coco + 1e-9), 4)\n",
        "                    else: metrics[\"F1_coco\"] = -1.0\n",
        "                    coco_eval_stats_for_curves = coco_eval_obj_instance \n",
        "    if inference_times and num_processed_frames > 0: \n",
        "        total_inf_time = sum(inference_times)\n",
        "        fps = num_processed_frames / total_inf_time if total_inf_time > 0 else 0.0\n",
        "        metrics[\"FPS\"] = round(fps, 2)\n",
        "    \n",
        "    metrics['coco_eval_obj_for_curves'] = coco_eval_stats_for_curves \n",
        "    \n",
        "    print(\"Calculating P, R, F1 vs Confidence metrics...\")\n",
        "    confidence_thresholds_custom, p_per_class, r_per_class, f1_per_class, p_overall, r_overall, f1_overall = \\\n",
        "        calculate_metrics_vs_confidence(all_predictions_for_curves, all_targets_for_curves, \n",
        "                                        num_classes_no_bg=num_classes_no_bg_for_curves, \n",
        "                                        inv_class_mapping=inv_class_mapping_for_curves, \n",
        "                                        iou_threshold=iou_threshold)\n",
        "    metrics['custom_metrics_data'] = {\n",
        "        'conf_thresholds': confidence_thresholds_custom,\n",
        "        'p_per_class': p_per_class, 'r_per_class': r_per_class, 'f1_per_class': f1_per_class,\n",
        "        'p_overall': p_overall, 'r_overall': r_overall, 'f1_overall': f1_overall\n",
        "    }\n",
        "    print(\"Finished calculating P, R, F1 vs Confidence metrics.\")\n",
        "    \n",
        "    if f1_overall and any(f > 0 for f in f1_overall):\n",
        "        best_f1_idx_overall = np.argmax(f1_overall)\n",
        "        optimal_confidence_threshold_for_cm = confidence_thresholds_custom[best_f1_idx_overall]\n",
        "        print(f\"Using optimal confidence threshold for CM: {optimal_confidence_threshold_for_cm:.3f} (maximizes overall F1)\")\n",
        "\n",
        "    for i in range(len(all_predictions_for_curves)):\n",
        "        preds_img = all_predictions_for_curves[i]\n",
        "        targets_img = all_targets_for_curves[i]\n",
        "        gt_boxes_img = targets_img['boxes'].cpu().numpy()\n",
        "        gt_labels_img = targets_img['labels'].cpu().numpy()\n",
        "        pred_scores_img = preds_img['scores'].cpu().numpy()\n",
        "        confident_preds_mask = pred_scores_img >= optimal_confidence_threshold_for_cm\n",
        "        pred_boxes_img_conf = preds_img['boxes'].cpu().numpy()[confident_preds_mask]\n",
        "        pred_labels_img_conf = preds_img['labels'].cpu().numpy()[confident_preds_mask]\n",
        "\n",
        "        matched_gt_for_cm = [False] * len(gt_boxes_img)\n",
        "        for pred_idx in range(len(pred_boxes_img_conf)):\n",
        "            pred_box = pred_boxes_img_conf[pred_idx]\n",
        "            pred_label = pred_labels_img_conf[pred_idx]\n",
        "            best_iou_cm = 0\n",
        "            best_gt_idx_cm = -1\n",
        "            for gt_idx in range(len(gt_boxes_img)):\n",
        "                if not matched_gt_for_cm[gt_idx]:\n",
        "                    iou = calculate_iou(pred_box, gt_boxes_img[gt_idx])\n",
        "                    if iou > best_iou_cm:\n",
        "                        best_iou_cm = iou\n",
        "                        best_gt_idx_cm = gt_idx\n",
        "            \n",
        "            if best_iou_cm >= iou_threshold:\n",
        "                if gt_labels_img[best_gt_idx_cm] == pred_label:\n",
        "                    all_gt_labels_for_cm.append(gt_labels_img[best_gt_idx_cm])\n",
        "                    all_pred_labels_for_cm.append(pred_label)\n",
        "                else:\n",
        "                    all_gt_labels_for_cm.append(gt_labels_img[best_gt_idx_cm])\n",
        "                    all_pred_labels_for_cm.append(pred_label)\n",
        "                matched_gt_for_cm[best_gt_idx_cm] = True\n",
        "            else:\n",
        "                if pred_label != CLASS_MAPPING['background']:\n",
        "                    all_gt_labels_for_cm.append(CLASS_MAPPING['background'])\n",
        "                    all_pred_labels_for_cm.append(pred_label)\n",
        "        \n",
        "        for gt_idx in range(len(gt_boxes_img)):\n",
        "            if not matched_gt_for_cm[gt_idx]:\n",
        "                if gt_labels_img[gt_idx] != CLASS_MAPPING['background']:\n",
        "                    all_gt_labels_for_cm.append(gt_labels_img[gt_idx])\n",
        "                    all_pred_labels_for_cm.append(CLASS_MAPPING['background'])\n",
        "                \n",
        "    metrics['cm_data'] = {'true': all_gt_labels_for_cm, 'pred': all_pred_labels_for_cm}\n",
        "                \n",
        "    return metrics\n",
        "\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    import torchvision \n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN): iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN): iou_types.append(\"keypoints\")\n",
        "    return iou_types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c65da1",
      "metadata": {},
      "source": [
        "## 4.1 Functions for Visualizing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d574fa6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def denormalize_image_tensor(tensor, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)):\n",
        "    \"\"\"Denormalizes a tensor image with mean and standard deviation.\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Tensor image of size (C, H, W) to be denormalized.\n",
        "        mean (tuple): Mean for each channel.\n",
        "        std (tuple): Standard deviation for each channel.\n",
        "    Returns:\n",
        "        torch.Tensor: Denormalized tensor image.\n",
        "    \"\"\"\n",
        "    tensor = tensor.clone() \n",
        "    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
        "    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device).view(-1, 1, 1)\n",
        "    tensor.mul_(std).add_(mean) \n",
        "    return torch.clamp(tensor, 0, 1) \n",
        "\n",
        "def plot_coco_evaluation_curves(coco_eval_obj, save_dir, class_names_dict_inv):\n",
        "    if coco_eval_obj is None or not hasattr(coco_eval_obj, 'eval') or not coco_eval_obj.eval or 'precision' not in coco_eval_obj.eval:\n",
        "        print(\"No COCO evaluation data to draw standard PR curve.\")\n",
        "        return {}\n",
        "    precision = coco_eval_obj.eval['precision'] \n",
        "    recall_thresholds = coco_eval_obj.params.recThrs \n",
        "    \n",
        "    pr_path = os.path.join(save_dir, \"PR_curve.png\")\n",
        "    plt.figure(figsize=(12, 9)) \n",
        "    \n",
        "    if precision.shape[0] > 0 and precision.shape[2] > 0: \n",
        "        mean_precisions_iou05 = np.mean(precision[0, :, :, 0, 2], axis=1) \n",
        "        plt.plot(recall_thresholds, mean_precisions_iou05, color='navy', lw=3, \n",
        "                 label=f'Average for all classes (mAP@0.5 = {coco_eval_obj.stats[1]:.3f})')\n",
        "\n",
        "    cat_ids_in_eval = coco_eval_obj.params.catIds\n",
        "    if not isinstance(class_names_dict_inv, dict): class_names_dict_inv = {}\n",
        "    \n",
        "    num_categories_in_precision_matrix = precision.shape[2] \n",
        "    \n",
        "    try:\n",
        "        colors = plt.cm.get_cmap('tab10', num_categories_in_precision_matrix)\n",
        "    except AttributeError: \n",
        "        colors = plt.cm.tab10\n",
        "\n",
        "    for k_idx in range(num_categories_in_precision_matrix):\n",
        "        if k_idx < len(cat_ids_in_eval): \n",
        "            cat_id = cat_ids_in_eval[k_idx]\n",
        "            class_name = class_names_dict_inv.get(cat_id, f\"ClsID {cat_id}\")\n",
        "            if class_name == 'background': continue\n",
        "            precision_for_cat = precision[0, :, k_idx, 0, 2]\n",
        "            ap_for_cat_iou05 = np.mean(precision_for_cat[precision_for_cat > -1]) if np.any(precision_for_cat > -1) else 0.0\n",
        "            current_color = colors(k_idx % 10) if callable(colors) else colors.colors[k_idx % len(colors.colors)]\n",
        "            plt.plot(recall_thresholds, precision_for_cat, color=current_color, lw=1.5, \n",
        "                     label=f'{class_name} (AP@0.5 = {ap_for_cat_iou05:.3f})')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve for IoU=0.5 (from COCOeval)')\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), borderaxespad=0., fontsize='small')\n",
        "    plt.grid(True)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.tight_layout(rect=[0,0,0.78,1]) \n",
        "    plt.savefig(pr_path)\n",
        "    plt.close()\n",
        "    print(f\"Saved standard PR_curve (from COCOeval) to {pr_path}\")\n",
        "    return {\"pr_curve\": pr_path}\n",
        "\n",
        "def plot_training_summary_curves(metrics_df, output_dir):\n",
        "    if 'epoch' not in metrics_df.columns:\n",
        "        print(\"Missing 'epoch' column in metrics_df. Cannot generate summary.\")\n",
        "        return\n",
        "    epochs = metrics_df['epoch']\n",
        "\n",
        "    metrics_to_plot_ordered = [\n",
        "        (\"train/box_loss\", \"loss_box_reg\"),\n",
        "        (\"train/cls_loss\", \"loss_classifier\"),\n",
        "        (\"train/obj_loss\", \"loss_objectness\"),\n",
        "        (\"metrics/precision\", \"Precision_coco\"),\n",
        "        (\"metrics/recall\", \"Recall_coco\"),\n",
        "        (\"train/box_loss_RPN\", \"loss_rpn_box_reg\"),\n",
        "        (\"train/loss_overall\", \"loss\"),\n",
        "        (\"metrics/F1-score\", \"F1_coco\"),\n",
        "        (\"metrics/mAP50\", \"mAP_0.5\"),\n",
        "        (\"metrics/mAP50-95\", \"mAP_0.5_0.95\")\n",
        "    ]\n",
        "\n",
        "    num_plots_defined = len(metrics_to_plot_ordered)\n",
        "    num_rows_plot = 2\n",
        "    num_cols_plot = 5\n",
        "\n",
        "    fig, axs = plt.subplots(num_rows_plot, num_cols_plot, figsize=(6 * num_cols_plot, 5 * num_rows_plot), squeeze=False)\n",
        "    axs = axs.flatten()\n",
        "    fig.suptitle('Faster R-CNN Training & Validation Summary', fontsize=18)\n",
        "\n",
        "    plot_idx = 0\n",
        "    for plot_title, df_col_name in metrics_to_plot_ordered:\n",
        "        if df_col_name in metrics_df.columns and metrics_df[df_col_name].notna().any():\n",
        "            if plot_idx < len(axs):\n",
        "                axs[plot_idx].plot(epochs, metrics_df[df_col_name], label=plot_title.split('/')[-1], marker='.')\n",
        "                axs[plot_idx].set_title(plot_title, fontsize=10)\n",
        "                axs[plot_idx].set_xlabel('Epoch')\n",
        "                axs[plot_idx].grid(True)\n",
        "                axs[plot_idx].legend(fontsize='small')\n",
        "                plot_idx += 1\n",
        "            else:\n",
        "                print(f\"Warning: Not enough subplot space for {plot_title}. Check grid configuration.\")\n",
        "                break \n",
        "        else:\n",
        "            print(f\"Warning: Metric '{df_col_name}' for title '{plot_title}' not found in metrics_df or contains only NaNs. Skipping this plot.\")\n",
        "            if plot_idx < len(axs):\n",
        "                axs[plot_idx].axis('off')\n",
        "                axs[plot_idx].set_title(f'{plot_title}\\n(No data)', fontsize=9, color='grey')\n",
        "                axs[plot_idx].grid(True)\n",
        "                plot_idx += 1 \n",
        "    \n",
        "    for i in range(plot_idx, len(axs)):\n",
        "        axs[i].axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    save_path = os.path.join(output_dir, \"faster_rcnn_metrics_summary.png\") \n",
        "    plt.savefig(save_path); plt.close(fig)\n",
        "    print(f\"Saved metrics summary (plots) to {save_path}\")\n",
        "\n",
        "def draw_predictions_on_image(image_pil, gt_boxes, gt_labels, pred_boxes, pred_labels, pred_scores, class_mapping_inv, score_thresh=0.3):\n",
        "    draw = ImageDraw.Draw(image_pil)\n",
        "    try: font = ImageFont.truetype(\"arial.ttf\", 12) \n",
        "    except IOError: font = ImageFont.load_default() \n",
        "    if gt_boxes is not None and gt_labels is not None:\n",
        "        for box, label_id in zip(gt_boxes, gt_labels):\n",
        "            if isinstance(box, torch.Tensor): box = box.tolist()\n",
        "            if isinstance(label_id, torch.Tensor): label_id = label_id.item()\n",
        "            draw.rectangle(box, outline=\"lime\", width=2) \n",
        "            label_text = class_mapping_inv.get(label_id, str(label_id))\n",
        "            draw.text((box[0], box[1] - 12 if box[1] > 12 else box[1] + 1), f\"GT: {label_text}\", fill=\"lime\", font=font)\n",
        "    if pred_boxes is not None and pred_labels is not None and pred_scores is not None:\n",
        "        for box, label_id, score in zip(pred_boxes, pred_labels, pred_scores):\n",
        "            if isinstance(box, torch.Tensor): box = box.tolist()\n",
        "            if isinstance(label_id, torch.Tensor): label_id = label_id.item()\n",
        "            if isinstance(score, torch.Tensor): score = score.item()\n",
        "            if score < score_thresh: continue \n",
        "            draw.rectangle(box, outline=\"red\", width=2) \n",
        "            label_text = class_mapping_inv.get(label_id, str(label_id))\n",
        "            draw.text((box[0], box[1] - 24 if box[1] > 24 else box[1] + 10), f\"P: {label_text} {score:.2f}\", fill=\"red\", font=font)\n",
        "    return image_pil\n",
        "\n",
        "def save_prediction_examples(model, data_loader, device, num_images_to_save, output_dir, class_mapping_inv, epoch_num_str): \n",
        "    model.eval()\n",
        "    saved_count = 0\n",
        "    images_to_collate = []\n",
        "    imagenet_mean = [0.485, 0.456, 0.406]\n",
        "    imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    if data_loader is None or (hasattr(data_loader, 'dataset') and len(data_loader.dataset) == 0):\n",
        "        print(\"No data in data_loader for save_prediction_examples.\")\n",
        "        return\n",
        "\n",
        "    data_iter = iter(data_loader)\n",
        "    batch_size = data_loader.batch_size if data_loader and hasattr(data_loader, 'batch_size') and data_loader.batch_size else 1\n",
        "    if batch_size == 0: batch_size = 1 \n",
        "    batches_to_process = (num_images_to_save + batch_size - 1) // batch_size\n",
        "\n",
        "    for _ in tqdm(range(batches_to_process), desc=f\"Generating prediction examples (epoch {epoch_num_str})\", leave=False):\n",
        "        try:\n",
        "            images_batch, targets_batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            break \n",
        "        if images_batch is None or targets_batch is None:\n",
        "            continue\n",
        "\n",
        "        images_for_model = list(img.to(device) for img in images_batch)\n",
        "        with torch.no_grad():\n",
        "            predictions = model(images_for_model)\n",
        "\n",
        "        for i in range(len(images_batch)):\n",
        "            if saved_count >= num_images_to_save:\n",
        "                break\n",
        "            img_tensor_normalized = images_batch[i].cpu()\n",
        "            img_tensor_denormalized = denormalize_image_tensor(img_tensor_normalized, mean=imagenet_mean, std=imagenet_std)\n",
        "            try:\n",
        "                pil_img = torchvision.transforms.ToPILImage()(img_tensor_denormalized)\n",
        "            except Exception as e_pil:\n",
        "                print(f\"Error converting tensor (after denormalization) to PIL: {e_pil}\")\n",
        "                continue\n",
        "            gt_boxes = targets_batch[i].get('boxes', torch.empty(0,4))\n",
        "            gt_labels = targets_batch[i].get('labels', torch.empty(0, dtype=torch.int64))\n",
        "            pred_boxes = predictions[i].get('boxes', torch.empty(0,4)).cpu()\n",
        "            pred_labels = predictions[i].get('labels', torch.empty(0, dtype=torch.int64)).cpu()\n",
        "            pred_scores = predictions[i].get('scores', torch.empty(0)).cpu()\n",
        "            drawn_image = draw_predictions_on_image(pil_img, gt_boxes, gt_labels, pred_boxes, pred_labels, pred_scores, class_mapping_inv)\n",
        "            images_to_collate.append(drawn_image)\n",
        "            saved_count += 1\n",
        "        if saved_count >= num_images_to_save:\n",
        "            break\n",
        "    \n",
        "    if not images_to_collate:\n",
        "        print(\"Failed to generate any images with predictions.\")\n",
        "        return\n",
        "\n",
        "    num_to_collate = min(len(images_to_collate), 16) \n",
        "    images_to_collate = images_to_collate[:num_to_collate]\n",
        "    if not images_to_collate: return\n",
        "\n",
        "    num_cols = 4\n",
        "    num_rows = (len(images_to_collate) + num_cols - 1) // num_cols\n",
        "    first_image_width = images_to_collate[0].width\n",
        "    first_image_height = images_to_collate[0].height\n",
        "    grid_img_width = first_image_width * num_cols\n",
        "    grid_img_height = first_image_height * num_rows\n",
        "    grid_image = Image.new('RGB', (grid_img_width, grid_img_height), color='white')\n",
        "\n",
        "    for idx, img_to_paste in enumerate(images_to_collate):\n",
        "        row_idx = idx // num_cols\n",
        "        col_idx = idx % num_cols\n",
        "        if img_to_paste.size != (first_image_width, first_image_height):\n",
        "            img_to_paste = img_to_paste.resize((first_image_width, first_image_height))\n",
        "        grid_image.paste(img_to_paste, (col_idx * first_image_width, row_idx * first_image_height))\n",
        "    save_path = os.path.join(output_dir, \"predictions.jpg\") \n",
        "    grid_image.save(save_path)\n",
        "    print(f\"Saved example predictions to {save_path}\")\n",
        "\n",
        "def save_ground_truth_examples(data_loader, device, num_images_to_save, output_dir, class_mapping_inv, epoch_num_str_for_desc=\"\"):\n",
        "    saved_count = 0\n",
        "    images_to_collate = []\n",
        "    imagenet_mean = [0.485, 0.456, 0.406]\n",
        "    imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    if data_loader is None or (hasattr(data_loader, 'dataset') and len(data_loader.dataset) == 0):\n",
        "        print(\"No data in data_loader for save_ground_truth_examples.\")\n",
        "        return\n",
        "\n",
        "    data_iter = iter(data_loader)\n",
        "    batch_size = data_loader.batch_size if data_loader and hasattr(data_loader, 'batch_size') and data_loader.batch_size else 1\n",
        "    if batch_size == 0: batch_size = 1\n",
        "    batches_to_process = (num_images_to_save + batch_size - 1) // batch_size\n",
        "    desc_str = \"Generating ground truth examples\"\n",
        "    if epoch_num_str_for_desc:\n",
        "        desc_str += f\" (epoch {epoch_num_str_for_desc})\"\n",
        "\n",
        "    for _ in tqdm(range(batches_to_process), desc=desc_str, leave=False):\n",
        "        try:\n",
        "            images_batch, targets_batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            break \n",
        "        if images_batch is None or targets_batch is None:\n",
        "            continue\n",
        "\n",
        "        for i in range(len(images_batch)):\n",
        "            if saved_count >= num_images_to_save:\n",
        "                break\n",
        "            img_tensor_normalized = images_batch[i].cpu()\n",
        "            img_tensor_denormalized = denormalize_image_tensor(img_tensor_normalized, mean=imagenet_mean, std=imagenet_std)\n",
        "            try:\n",
        "                pil_img = torchvision.transforms.ToPILImage()(img_tensor_denormalized)\n",
        "            except Exception as e_pil:\n",
        "                print(f\"Error converting tensor (after denormalization) to PIL in GT: {e_pil}\")\n",
        "                continue\n",
        "            gt_boxes = targets_batch[i].get('boxes', torch.empty(0,4))\n",
        "            gt_labels = targets_batch[i].get('labels', torch.empty(0, dtype=torch.int64))\n",
        "            drawn_image = draw_predictions_on_image(pil_img, gt_boxes, gt_labels, \n",
        "                                                    pred_boxes=None, pred_labels=None, pred_scores=None, \n",
        "                                                    class_mapping_inv=class_mapping_inv)\n",
        "            images_to_collate.append(drawn_image)\n",
        "            saved_count += 1\n",
        "        if saved_count >= num_images_to_save:\n",
        "            break\n",
        "    \n",
        "    if not images_to_collate:\n",
        "        print(\"Failed to generate any images with ground truth.\")\n",
        "        return\n",
        "\n",
        "    num_to_collate = min(len(images_to_collate), 16)\n",
        "    images_to_collate = images_to_collate[:num_to_collate]\n",
        "    if not images_to_collate: return\n",
        "\n",
        "    num_cols = 4\n",
        "    num_rows = (len(images_to_collate) + num_cols - 1) // num_cols\n",
        "    first_image_width = images_to_collate[0].width\n",
        "    first_image_height = images_to_collate[0].height\n",
        "    grid_img_width = first_image_width * num_cols\n",
        "    grid_img_height = first_image_height * num_rows\n",
        "    grid_image = Image.new('RGB', (grid_img_width, grid_img_height), color='white')\n",
        "\n",
        "    for idx, img_to_paste in enumerate(images_to_collate):\n",
        "        row_idx = idx // num_cols\n",
        "        col_idx = idx % num_cols\n",
        "        if img_to_paste.size != (first_image_width, first_image_height):\n",
        "            img_to_paste = img_to_paste.resize((first_image_width, first_image_height))\n",
        "        grid_image.paste(img_to_paste, (col_idx * first_image_width, row_idx * first_image_height))\n",
        "    save_path = os.path.join(output_dir, \"ground_truth_examples.jpg\") \n",
        "    grid_image.save(save_path)\n",
        "    print(f\"Saved example ground truth images to {save_path}\")\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"Calculates Intersection over Union (IoU) between two boxes.\n",
        "    Boxes are in [xmin, ymin, xmax, ymax] format.\n",
        "    \"\"\"\n",
        "    x1_inter = max(box1[0], box2[0])\n",
        "    y1_inter = max(box1[1], box2[1])\n",
        "    x2_inter = min(box1[2], box2[2])\n",
        "    y2_inter = min(box1[3], box2[3])\n",
        "    width_inter = max(0, x2_inter - x1_inter)\n",
        "    height_inter = max(0, y2_inter - y1_inter)\n",
        "    area_inter = width_inter * height_inter\n",
        "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    area_union = area_box1 + area_box2 - area_inter\n",
        "    iou = area_inter / area_union if area_union > 0 else 0.0\n",
        "    return iou\n",
        "\n",
        "def calculate_metrics_vs_confidence(all_predictions, all_targets, num_classes_no_bg, inv_class_mapping, iou_threshold=0.5, confidence_thresholds=None):\n",
        "    if confidence_thresholds is None:\n",
        "        confidence_thresholds = np.linspace(0.01, 1.0, 100) \n",
        "    actual_class_ids = [cls_id for cls_id in inv_class_mapping.keys() if cls_id != CLASS_MAPPING['background']] \n",
        "    precisions_per_class = {cls_id: [] for cls_id in actual_class_ids}\n",
        "    recalls_per_class = {cls_id: [] for cls_id in actual_class_ids}\n",
        "    f1_scores_per_class = {cls_id: [] for cls_id in actual_class_ids}\n",
        "    overall_precisions = []\n",
        "    overall_recalls = []\n",
        "    overall_f1_scores = []\n",
        "\n",
        "    for conf_thresh in tqdm(confidence_thresholds, desc=\"Calculating P/R/F1 vs Confidence\"):\n",
        "        total_tp_overall = 0; total_fp_overall = 0; total_fn_overall = 0\n",
        "        tp_per_class = {cls_id: 0 for cls_id in actual_class_ids}\n",
        "        fp_per_class = {cls_id: 0 for cls_id in actual_class_ids}\n",
        "        fn_per_class = {cls_id: 0 for cls_id in actual_class_ids}\n",
        "\n",
        "        for i in range(len(all_predictions)):\n",
        "            preds_img = all_predictions[i]; targets_img = all_targets[i]\n",
        "            gt_boxes_img = targets_img['boxes'].cpu().numpy(); gt_labels_img = targets_img['labels'].cpu().numpy()\n",
        "            pred_scores_img = preds_img['scores'].cpu().numpy()\n",
        "            confident_mask = pred_scores_img >= conf_thresh\n",
        "            pred_boxes_img_conf = preds_img['boxes'].cpu().numpy()[confident_mask]\n",
        "            pred_labels_img_conf = preds_img['labels'].cpu().numpy()[confident_mask]\n",
        "            pred_scores_img_conf = pred_scores_img[confident_mask]\n",
        "\n",
        "            if len(pred_boxes_img_conf) > 0:\n",
        "                sorted_indices = np.argsort(pred_scores_img_conf)[::-1]\n",
        "                pred_boxes_img_conf = pred_boxes_img_conf[sorted_indices]\n",
        "                pred_labels_img_conf = pred_labels_img_conf[sorted_indices]\n",
        "\n",
        "            matched_gt_indices = [False] * len(gt_boxes_img)\n",
        "            for pred_idx in range(len(pred_boxes_img_conf)):\n",
        "                pred_box = pred_boxes_img_conf[pred_idx]; pred_label = pred_labels_img_conf[pred_idx]\n",
        "                best_iou = 0; best_gt_idx = -1\n",
        "                for gt_idx in range(len(gt_boxes_img)):\n",
        "                    if gt_labels_img[gt_idx] == pred_label and not matched_gt_indices[gt_idx]:\n",
        "                        iou = calculate_iou(pred_box, gt_boxes_img[gt_idx])\n",
        "                        if iou > best_iou: best_iou = iou; best_gt_idx = gt_idx\n",
        "                if best_iou >= iou_threshold:\n",
        "                    if pred_label in tp_per_class: tp_per_class[pred_label] += 1\n",
        "                    total_tp_overall += 1; matched_gt_indices[best_gt_idx] = True\n",
        "                else:\n",
        "                    if pred_label in fp_per_class: fp_per_class[pred_label] += 1\n",
        "                    total_fp_overall += 1\n",
        "            for gt_idx in range(len(gt_boxes_img)):\n",
        "                if not matched_gt_indices[gt_idx]:\n",
        "                    gt_label = gt_labels_img[gt_idx]\n",
        "                    if gt_label in fn_per_class: fn_per_class[gt_label] += 1; total_fn_overall +=1\n",
        "\n",
        "        for cls_id in actual_class_ids:\n",
        "            p_cls = tp_per_class[cls_id] / (tp_per_class[cls_id] + fp_per_class[cls_id]) if (tp_per_class[cls_id] + fp_per_class[cls_id]) > 0 else 0\n",
        "            r_cls = tp_per_class[cls_id] / (tp_per_class[cls_id] + fn_per_class[cls_id]) if (tp_per_class[cls_id] + fn_per_class[cls_id]) > 0 else 0\n",
        "            f1_cls = 2 * p_cls * r_cls / (p_cls + r_cls) if (p_cls + r_cls) > 0 else 0\n",
        "            precisions_per_class[cls_id].append(p_cls); recalls_per_class[cls_id].append(r_cls); f1_scores_per_class[cls_id].append(f1_cls)\n",
        "        p_overall = total_tp_overall / (total_tp_overall + total_fp_overall) if (total_tp_overall + total_fp_overall) > 0 else 0\n",
        "        r_overall = total_tp_overall / (total_tp_overall + total_fn_overall) if (total_tp_overall + total_fn_overall) > 0 else 0\n",
        "        f1_overall = 2 * p_overall * r_overall / (p_overall + r_overall) if (p_overall + r_overall) > 0 else 0\n",
        "        overall_precisions.append(p_overall); overall_recalls.append(r_overall); overall_f1_scores.append(f1_overall)\n",
        "    return confidence_thresholds, precisions_per_class, recalls_per_class, f1_scores_per_class, overall_precisions, overall_recalls, overall_f1_scores\n",
        "\n",
        "def plot_custom_metric_curves(custom_metrics_data, save_dir, inv_class_mapping):\n",
        "    if not custom_metrics_data:\n",
        "        print(\"No data to draw custom metric curves.\"); return\n",
        "    conf_thresholds = custom_metrics_data['conf_thresholds']\n",
        "    p_per_class = custom_metrics_data['p_per_class']; r_per_class = custom_metrics_data['r_per_class']; f1_per_class = custom_metrics_data['f1_per_class']\n",
        "    p_overall = custom_metrics_data['p_overall']; r_overall = custom_metrics_data['r_overall']; f1_overall = custom_metrics_data['f1_overall']\n",
        "    actual_class_ids = [cls_id for cls_id in inv_class_mapping.keys() if cls_id != CLASS_MAPPING['background']]\n",
        "    num_actual_classes = len(actual_class_ids)\n",
        "    colors = plt.cm.get_cmap('tab10', num_actual_classes) if num_actual_classes > 0 else plt.cm.get_cmap('tab10')\n",
        "    \n",
        "    best_f1_idx_overall = -1\n",
        "    best_conf_for_f1 = -1\n",
        "    if f1_overall and any(f1 > 0 for f1 in f1_overall):\n",
        "        best_f1_idx_overall = np.argmax(f1_overall)\n",
        "        best_conf_for_f1 = conf_thresholds[best_f1_idx_overall]\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    for i, cls_id in enumerate(actual_class_ids):\n",
        "        plt.plot(conf_thresholds, p_per_class[cls_id], color=colors(i % 10), lw=1, label=f'{inv_class_mapping[cls_id]}')\n",
        "    if best_f1_idx_overall != -1:\n",
        "        best_p_overall = p_overall[best_f1_idx_overall]\n",
        "        plt.plot(conf_thresholds, p_overall, color='blue', lw=3, label=f'All classes {best_p_overall:.2f} at {best_conf_for_f1:.3f} conf (for max F1)')\n",
        "    else:\n",
        "        plt.plot(conf_thresholds, p_overall, color='blue', lw=3, label='All classes (no F1 data)')\n",
        "    plt.xlabel('Confidence Threshold'); plt.ylabel('Precision'); plt.title('Precision-Confidence Curve')\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5)); plt.grid(True); plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
        "    plt.tight_layout(rect=[0,0,0.8,1]); plt.savefig(os.path.join(save_dir, \"P_curve.png\")); plt.close()\n",
        "    print(f\"Saved P_curve.png to {save_dir}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    for i, cls_id in enumerate(actual_class_ids):\n",
        "        plt.plot(conf_thresholds, r_per_class[cls_id], color=colors(i % 10), lw=1, label=f'{inv_class_mapping[cls_id]}')\n",
        "    if best_f1_idx_overall != -1:\n",
        "        best_r_overall = r_overall[best_f1_idx_overall]\n",
        "        plt.plot(conf_thresholds, r_overall, color='blue', lw=3, label=f'All classes {best_r_overall:.2f} at {best_conf_for_f1:.3f} conf (for max F1)')\n",
        "    else:\n",
        "        plt.plot(conf_thresholds, r_overall, color='blue', lw=3, label='All classes (no F1 data)')\n",
        "    plt.xlabel('Confidence Threshold'); plt.ylabel('Recall'); plt.title('Recall-Confidence Curve')\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5)); plt.grid(True); plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
        "    plt.tight_layout(rect=[0,0,0.8,1]); plt.savefig(os.path.join(save_dir, \"R_curve.png\")); plt.close()\n",
        "    print(f\"Saved R_curve.png to {save_dir}\")\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    for i, cls_id in enumerate(actual_class_ids):\n",
        "        plt.plot(conf_thresholds, f1_per_class[cls_id], color=colors(i % 10), lw=1, label=f'{inv_class_mapping[cls_id]}')\n",
        "    if best_f1_idx_overall != -1:\n",
        "        best_f1_overall_val = f1_overall[best_f1_idx_overall]\n",
        "        plt.plot(conf_thresholds, f1_overall, color='blue', lw=3, label=f'All classes {best_f1_overall_val:.2f} at {best_conf_for_f1:.3f} conf')\n",
        "    else:\n",
        "        plt.plot(conf_thresholds, f1_overall, color='blue', lw=3, label='All classes (no F1 data)')\n",
        "    plt.xlabel('Confidence Threshold'); plt.ylabel('F1-Score'); plt.title('F1-Confidence Curve')\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5)); plt.grid(True); plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
        "    plt.tight_layout(rect=[0,0,0.8,1]); plt.savefig(os.path.join(save_dir, \"F1_curve.png\")); plt.close()\n",
        "    print(f\"Saved F1_curve.png to {save_dir}\")\n",
        "\n",
        "def plot_confusion_matrices(cm_data, class_names_no_bg, output_dir):\n",
        "    if not cm_data or not cm_data['true'] or not cm_data['pred']:\n",
        "        print(\"No data to generate confusion matrix.\")\n",
        "        return\n",
        "    \n",
        "    true_labels_int = cm_data['true']\n",
        "    pred_labels_int = cm_data['pred']\n",
        "    \n",
        "    all_possible_labels_ids = sorted(list(CLASS_MAPPING.values()))\n",
        "    all_possible_labels_names = [INV_CLASS_MAPPING.get(l_id, f'ID {l_id}') for l_id in all_possible_labels_ids]\n",
        "\n",
        "    if not true_labels_int and not pred_labels_int:\n",
        "        print(\"No labels (true or predicted) to generate confusion matrix.\")\n",
        "        cm = np.zeros((len(all_possible_labels_ids), len(all_possible_labels_ids)), dtype=int)\n",
        "    else:\n",
        "        cm = confusion_matrix(true_labels_int, pred_labels_int, labels=all_possible_labels_ids)\n",
        "\n",
        "    plt.figure(figsize=(12, 10)) \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=all_possible_labels_names, yticklabels=all_possible_labels_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"))\n",
        "    plt.close()\n",
        "    print(f\"Saved confusion_matrix.png to {output_dir}\")\n",
        "\n",
        "    cm_sum_axis1 = cm.sum(axis=1)[:, np.newaxis]\n",
        "    cm_normalized = np.zeros_like(cm, dtype=float)\n",
        "    np.divide(cm, cm_sum_axis1, out=cm_normalized, where=cm_sum_axis1!=0)\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=all_possible_labels_names, yticklabels=all_possible_labels_names)\n",
        "    plt.title('Normalized Confusion Matrix (by true labels)')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(output_dir, \"confusion_matrix_normalized.png\"))\n",
        "    plt.close()\n",
        "    print(f\"Saved confusion_matrix_normalized.png to {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32cba668",
      "metadata": {},
      "source": [
        "## 5. Main Training Script (Combined Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8e42e3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"--- Training Faster R-CNN on combined data (Day/Night) ---\")\n",
        "\n",
        "if not os.path.exists(TRAIN_ANNOTATIONS_FILE) or (os.path.exists(TRAIN_ANNOTATIONS_FILE) and os.path.getsize(TRAIN_ANNOTATIONS_FILE) < 50):\n",
        "    print(f\"CRITICAL ERROR: Training annotations file {TRAIN_ANNOTATIONS_FILE} does not exist or is empty.\")\n",
        "elif not os.path.exists(VAL_ANNOTATIONS_FILE) or (os.path.exists(VAL_ANNOTATIONS_FILE) and os.path.getsize(VAL_ANNOTATIONS_FILE) < 50):\n",
        "    print(f\"WARNING: Validation annotations file {VAL_ANNOTATIONS_FILE} does not exist or is empty. Evaluation may not be possible.\")\n",
        "\n",
        "dataset_train_full = LISADataset(annotations_file=TRAIN_ANNOTATIONS_FILE,\n",
        "                                   img_base_dir=IMAGES_BASE_DIR, \n",
        "                                   transforms=get_train_transforms(apply_augmentations=APPLY_AUGMENTATIONS), \n",
        "                                   class_mapping=CLASS_MAPPING)\n",
        "\n",
        "dataset_val_full = LISADataset(annotations_file=VAL_ANNOTATIONS_FILE,\n",
        "                                 img_base_dir=IMAGES_BASE_DIR, \n",
        "                                 transforms=get_val_test_transforms(), \n",
        "                                 class_mapping=CLASS_MAPPING)\n",
        "\n",
        "final_dataset_train = dataset_train_full \n",
        "final_dataset_val = dataset_val_full    \n",
        "\n",
        "if USE_SUBSET_DATA:\n",
        "    len_train_full = len(dataset_train_full) if dataset_train_full and hasattr(dataset_train_full, 'image_filenames') and len(dataset_train_full.image_filenames)>0 else 0\n",
        "    len_val_full = len(dataset_val_full) if dataset_val_full and hasattr(dataset_val_full, 'image_filenames') and len(dataset_val_full.image_filenames)>0 else 0\n",
        "    train_indices = torch.randperm(len_train_full)[:SUBSET_SIZE_TRAIN].tolist() if len_train_full > SUBSET_SIZE_TRAIN and len_train_full > 0 else list(range(len_train_full))\n",
        "    val_indices = torch.randperm(len_val_full)[:SUBSET_SIZE_VAL].tolist() if len_val_full > SUBSET_SIZE_VAL and len_val_full > 0 else list(range(len_val_full))\n",
        "    if not train_indices and len_train_full > 0: final_dataset_train = dataset_train_full \n",
        "    elif len_train_full == 0: final_dataset_train = dataset_train_full \n",
        "    else: final_dataset_train = Subset(dataset_train_full, train_indices)\n",
        "    if not val_indices and len_val_full > 0: final_dataset_val = dataset_val_full \n",
        "    elif len_val_full == 0: final_dataset_val = dataset_val_full \n",
        "    else: final_dataset_val = Subset(dataset_val_full, val_indices)\n",
        "    print(f\"Using subset of data: {len(final_dataset_train)} training, {len(final_dataset_val)} validation.\")\n",
        "else:\n",
        "    print(f\"Using full dataset: {len(final_dataset_train)} training, {len(final_dataset_val)} validation.\")\n",
        "\n",
        "if APPLY_AUGMENTATIONS:\n",
        "    print(\"Augmentations for the training set ARE ENABLED.\")\n",
        "else:\n",
        "    print(\"Augmentations for the training set ARE DISABLED.\")\n",
        "\n",
        "if len(final_dataset_train) == 0:\n",
        "    print(\"CRITICAL ERROR: Training set is empty. Training cannot start.\")\n",
        "else:\n",
        "    data_loader_train = DataLoader(final_dataset_train, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "    data_loader_val = None\n",
        "    if len(final_dataset_val) > 0:\n",
        "        data_loader_val = DataLoader(final_dataset_val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "    else: print(\"Validation set is empty, DataLoader for validation will not be created.\")\n",
        "\n",
        "    model = get_faster_rcnn_model(num_classes_incl_background=NUM_CLASSES_INC_BG, backbone_name=\"resnet50\")\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) \n",
        "\n",
        "    all_metrics_history = []\n",
        "    best_map_metric = -1.0\n",
        "    epochs_without_improvement = 0\n",
        "    last_eval_metrics_for_plotting = {} \n",
        "\n",
        "    for epoch_idx in range(NUM_EPOCHS):\n",
        "        current_epoch_num = epoch_idx + 1\n",
        "        avg_train_losses_dict = train_one_epoch(model, optimizer, data_loader_train, DEVICE, current_epoch_num, print_freq=PRINT_FREQ) \n",
        "        \n",
        "        current_epoch_metrics_row = {\"epoch\": current_epoch_num}\n",
        "        for loss_name, loss_val in avg_train_losses_dict.items():\n",
        "            current_epoch_metrics_row[loss_name] = loss_val \n",
        "\n",
        "        eval_metrics_dict = {}\n",
        "        if data_loader_val and len(final_dataset_val) > 0:\n",
        "            print(f\"--- Epoch {current_epoch_num} Evaluation ---\")\n",
        "            eval_metrics_dict = evaluate(model, data_loader_val, DEVICE, CLASS_MAPPING, \n",
        "                                         COCO_UTILS_AVAILABLE, METRIC_LOGGER_AVAILABLE,\n",
        "                                         NUM_CLASSES_NO_BG, INV_CLASS_MAPPING) \n",
        "            last_eval_metrics_for_plotting = eval_metrics_dict \n",
        "            \n",
        "            current_epoch_metrics_row[\"mAP_0.5\"] = eval_metrics_dict.get(\"mAP_0.5\", -1.0)\n",
        "            current_epoch_metrics_row[\"mAP_0.5_0.95\"] = eval_metrics_dict.get(\"mAP_0.5_0.95\", -1.0)\n",
        "            current_epoch_metrics_row[\"Precision_coco\"] = eval_metrics_dict.get(\"Precision_coco\", -1.0)\n",
        "            current_epoch_metrics_row[\"Recall_coco\"] = eval_metrics_dict.get(\"Recall_coco\", -1.0)\n",
        "            current_epoch_metrics_row[\"F1_coco\"] = eval_metrics_dict.get(\"F1_coco\", -1.0)\n",
        "            current_epoch_metrics_row[\"FPS\"] = eval_metrics_dict.get(\"FPS\", -1.0)\n",
        "            current_epoch_metrics_row[\"coco_eval_obj_for_curves\"] = eval_metrics_dict.get('coco_eval_obj_for_curves', None)\n",
        "            current_epoch_metrics_row[\"custom_metrics_data\"] = eval_metrics_dict.get('custom_metrics_data', None)\n",
        "            current_epoch_metrics_row[\"cm_data\"] = eval_metrics_dict.get('cm_data', None) \n",
        "\n",
        "            all_metrics_history.append(current_epoch_metrics_row)\n",
        "            current_map_val = eval_metrics_dict.get(\"mAP_0.5\", -1.0)\n",
        "            if current_map_val > best_map_metric:\n",
        "                best_map_metric = current_map_val\n",
        "                epochs_without_improvement = 0\n",
        "                model_save_path = os.path.join(OUTPUT_DIR, \"faster_rcnn_best_model.pth\")\n",
        "                torch.save(model.state_dict(), model_save_path)\n",
        "                print(f\"Epoch {current_epoch_num}: New best model saved to {model_save_path} with mAP@0.5: {best_map_metric:.4f}\")\n",
        "            else:\n",
        "                epochs_without_improvement += 1\n",
        "                print(f\"Epoch {current_epoch_num}: No improvement in mAP@0.5. Best: {best_map_metric:.4f}. Epochs without improvement: {epochs_without_improvement}\")\n",
        "            if epochs_without_improvement >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"Early stopping! No improvement in mAP@0.5 for {EARLY_STOPPING_PATIENCE} epochs.\")\n",
        "                break \n",
        "        else:\n",
        "            all_metrics_history.append(current_epoch_metrics_row) \n",
        "\n",
        "        lr_scheduler.step() \n",
        "\n",
        "    if all_metrics_history: \n",
        "        metrics_df = pd.DataFrame(all_metrics_history)\n",
        "        coco_eval_object_for_plotting = None\n",
        "        custom_metrics_data_for_plotting = None\n",
        "        cm_data_for_plotting = None\n",
        "\n",
        "        if 'coco_eval_obj_for_curves' in metrics_df.columns:\n",
        "            valid_coco_evals = metrics_df['coco_eval_obj_for_curves'].dropna()\n",
        "            if not valid_coco_evals.empty:\n",
        "                coco_eval_object_for_plotting = valid_coco_evals.iloc[-1]\n",
        "        if 'custom_metrics_data' in metrics_df.columns:\n",
        "            valid_custom_metrics = metrics_df['custom_metrics_data'].dropna()\n",
        "            if not valid_custom_metrics.empty:\n",
        "                custom_metrics_data_for_plotting = valid_custom_metrics.iloc[-1]\n",
        "        if 'cm_data' in metrics_df.columns: \n",
        "            valid_cm_data = metrics_df['cm_data'].dropna()\n",
        "            if not valid_cm_data.empty:\n",
        "                cm_data_for_plotting = valid_cm_data.iloc[-1]\n",
        "\n",
        "        cols_to_drop_for_csv = ['coco_eval_obj_for_curves', 'custom_metrics_data', 'cm_data'] \n",
        "        metrics_df_for_csv = metrics_df.drop(columns=cols_to_drop_for_csv, errors='ignore')\n",
        "        \n",
        "        metrics_filename = os.path.join(OUTPUT_DIR, \"faster_rcnn_training_metrics.csv\")\n",
        "        metrics_df_for_csv.to_csv(metrics_filename, index=False)\n",
        "        print(f\"Training metrics saved to {metrics_filename}\")\n",
        "        \n",
        "        plot_training_summary_curves(metrics_df, OUTPUT_DIR) \n",
        "\n",
        "        if coco_eval_object_for_plotting: \n",
        "            plot_coco_evaluation_curves(coco_eval_object_for_plotting, VISUALIZATIONS_DIR, INV_CLASS_MAPPING)\n",
        "        elif last_eval_metrics_for_plotting.get('coco_eval_obj_for_curves'): \n",
        "             plot_coco_evaluation_curves(last_eval_metrics_for_plotting['coco_eval_obj_for_curves'], VISUALIZATIONS_DIR, INV_CLASS_MAPPING)\n",
        "        else:\n",
        "            print(\"COCOeval object not found to generate standard PR curve.\")\n",
        "        \n",
        "        if custom_metrics_data_for_plotting:\n",
        "            plot_custom_metric_curves(custom_metrics_data_for_plotting, VISUALIZATIONS_DIR, INV_CLASS_MAPPING)\n",
        "        elif last_eval_metrics_for_plotting.get('custom_metrics_data'):\n",
        "            plot_custom_metric_curves(last_eval_metrics_for_plotting.get('custom_metrics_data'), VISUALIZATIONS_DIR, INV_CLASS_MAPPING)\n",
        "        else:\n",
        "            print(\"Data not found to generate P, R, F1 vs Confidence curves.\")\n",
        "        \n",
        "        if cm_data_for_plotting:\n",
        "            plot_confusion_matrices(cm_data_for_plotting, CLASS_NAMES_NO_BG, VISUALIZATIONS_DIR)\n",
        "        elif last_eval_metrics_for_plotting.get('cm_data'):\n",
        "            plot_confusion_matrices(last_eval_metrics_for_plotting.get('cm_data'), CLASS_NAMES_NO_BG, VISUALIZATIONS_DIR)\n",
        "        else:\n",
        "            print(\"Data not found to generate confusion matrices.\")\n",
        "\n",
        "    else: print(\"No metrics collected to save.\")\n",
        "    \n",
        "    if data_loader_val and len(final_dataset_val) > 0:\n",
        "        print(\"\\nSaving example predictions and ground truth from the last/best model...\")\n",
        "        best_model_path = os.path.join(OUTPUT_DIR, \"faster_rcnn_best_model.pth\")\n",
        "        final_epoch_num_str_desc = str(all_metrics_history[-1]['epoch']) if all_metrics_history else 'final'\n",
        "        \n",
        "        vis_model = get_faster_rcnn_model(num_classes_incl_background=NUM_CLASSES_INC_BG, backbone_name=\"resnet50\")\n",
        "        if os.path.exists(best_model_path):\n",
        "            print(f\"Loading best model from {best_model_path} for visualization.\")\n",
        "            vis_model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
        "        else: \n",
        "            print(\"Using last model state from training loop for visualization ('best_model.pth' not found).\")\n",
        "            vis_model.load_state_dict(model.state_dict()) \n",
        "        vis_model.to(DEVICE)\n",
        "        vis_model.eval() \n",
        "\n",
        "        save_prediction_examples(vis_model, data_loader_val, DEVICE, \n",
        "                                 num_images_to_save=16, \n",
        "                                 output_dir=VISUALIZATIONS_DIR, \n",
        "                                 class_mapping_inv=INV_CLASS_MAPPING, \n",
        "                                 epoch_num_str=final_epoch_num_str_desc) \n",
        "\n",
        "        save_ground_truth_examples(data_loader_val, DEVICE, \n",
        "                                   num_images_to_save=16, \n",
        "                                   output_dir=VISUALIZATIONS_DIR, \n",
        "                                   class_mapping_inv=INV_CLASS_MAPPING,\n",
        "                                   epoch_num_str_for_desc=final_epoch_num_str_desc)\n",
        "\n",
        "\n",
        "print(\"\\n--- Faster R-CNN training completed ---\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
